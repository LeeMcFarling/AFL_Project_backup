{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee72ba79",
   "metadata": {},
   "source": [
    "# **Week 4 - Capstone Development**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83881a9",
   "metadata": {},
   "source": [
    "#### **Logistic Regression and Feature Scaling:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc4f094",
   "metadata": {},
   "source": [
    "So this is awkward, but I already did Logistic Regression and feature scaling in week 2. Furthermore, in week 3, I did a forward feature selection; a backward feature selection; and then a joined dataset where the numerical columns were seperated and then a PCA was performed on the data. From there, they were paired back up with the one-hot encoded data. \n",
    "\n",
    "In this week, we will find the featureset to rule them all :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "495a3c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from itertools import chain, combinations\n",
    "\n",
    "# Data Science Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.ticker as mticker  # Optional: Format y-axis labels as dollars\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Scikit-learn (Machine Learning)\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_val_score,\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    RepeatedStratifiedKFold,\n",
    "    RepeatedKFold\n",
    ")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error, accuracy_score, f1_score, roc_auc_score, balanced_accuracy_score\n",
    "from sklearn.feature_selection import SequentialFeatureSelector, f_regression, SelectKBest\n",
    "from sklearn.linear_model import LogisticRegression, Lasso, RidgeClassifier, ElasticNet\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_predict, KFold\n",
    "# Progress Tracking\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =============================\n",
    "# Global Variables\n",
    "# =============================\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8989cf6e",
   "metadata": {},
   "source": [
    "#### **Dataset Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41ce00cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BDB_All_Plays_Model_Ready = pd.read_csv(\"/Users/leemcfarling/projects/ds_homework/AFL_Project/AFL_Final_Project/BDB_All_Plays_Model_Ready.csv\") # Big Data Bowl Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998b8060",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDA_PCA_Features = pd.read_csv('../../Feature_Subsets/PDA_PCA_Features.csv')\n",
    "FNF_PCA_Features = pd.read_csv('../../Feature_Subsets/FNF_PCA_Features.csv')\n",
    "BDB_PCA_Features = pd.read_csv('../../Feature_Subsets/BDB_PCA_Features.csv')\n",
    "\n",
    "# === Backward Feature Sets ===\n",
    "FNF_back_Features = pd.read_csv('../../Feature_Subsets/FNF_back_Features.csv')\n",
    "PDA_back_Features = pd.read_csv('../../Feature_Subsets/PDA_back_Features.csv')\n",
    "BDB_back_Features = pd.read_csv('../../Feature_Subsets/BDB_back_Features.csv')\n",
    "\n",
    "# === Forward Feature Sets ===\n",
    "FNF_Forward_Features = pd.read_csv('../../Feature_Subsets/FNF_Forward_Features.csv')\n",
    "PDA_Forward_Features = pd.read_csv('../../Feature_Subsets/PDA_Forward_Features.csv')\n",
    "BDB_Forward_Features = pd.read_csv('../../Feature_Subsets/BDB_Forward_Features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde85da6",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6676e46b",
   "metadata": {},
   "source": [
    "#### **Function Definitions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5975c901",
   "metadata": {},
   "source": [
    "Function to take a provided dataframe and split that dataframe into feature and target columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fb1fdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================================\n",
    "# Function taken from Module 3 Final Project\n",
    "# https://github.com/LeeMcFarling/Final_Project_Writeup/blob/main/Final_Project_Report.ipynb\n",
    "# ===========================================================================================\n",
    "\n",
    "def train_test_split_data(df, target_col):\n",
    "    X = df.drop(columns=target_col)\n",
    "    y = df[target_col]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d9c084",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e968f6d",
   "metadata": {},
   "source": [
    "#### **Lists to split Data into Numeric and Categorical Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015dfb19",
   "metadata": {},
   "source": [
    "Because we already made one hot encoded variables here are lists to seperate numeric and categorical data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4b41fe",
   "metadata": {},
   "source": [
    "#### **Standardization Function**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc3c144",
   "metadata": {},
   "source": [
    "So, we would typically apply a standardization function here, but because all of our datasets are already pre-processed, there is really no need to. (They were already standardized before the feature selection and PCA pipelines), so now we just need to train test split each one and then go from there. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a2fed3",
   "metadata": {},
   "source": [
    "#### **Run Model Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6dd93ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================================\n",
    "# Taken from Mod 3 Week 8:\n",
    "# https://github.com/waysnyder/Module-3-Assignments/blob/main/Homework_08.ipynb\n",
    "# \n",
    "# Global dataframe logic taken from mod 3 final project: \n",
    "# https://github.com/LeeMcFarling/Final_Project_Writeup/blob/main/Final_Project_Report.ipynb\n",
    "# \n",
    "# Final Function was developed in Week 2 of this Module\n",
    "# =============================================================================================\n",
    "\n",
    "def run_model_classifier(model, X_train, y_train, X_test, y_test, n_repeats=10, n_jobs=-1, run_comment=None, return_model=False, concat_results=False, **model_params):\n",
    "\n",
    "    global combined_results\n",
    "    # Remove extra key used to store error metric, if it was added to the parameter dictionary\n",
    "    if 'accuracy_found' in model_params:\n",
    "        model_params = model_params.copy()\n",
    "        model_params.pop('accuracy_found', None)  \n",
    "        \n",
    "    # Instantiate the model if a class is provided\n",
    "    if isinstance(model, type):\n",
    "        model = model(**model_params)\n",
    "    else:                                    \n",
    "        model.set_params(**model_params)    \n",
    "\n",
    "    model_name = model.__name__ if isinstance(model, type) else model.__class__.__name__ # Added because \n",
    "\n",
    "\n",
    "    # Use RepeatedStratifiedKFold for classification to preserve class distribution\n",
    "    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=n_repeats, random_state=42)\n",
    "    \n",
    "    # Perform 5-fold cross-validation using accuracy as the scoring metric\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=n_jobs)\n",
    "    \n",
    "    mean_cv_accuracy = np.mean(cv_scores)\n",
    "    std_cv_accuracy  = np.std(cv_scores)\n",
    "    \n",
    "    # Fit the model on the full training set\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Compute training and testing accuracy\n",
    "    train_preds    = model.predict(X_train)\n",
    "    test_preds     = model.predict(X_test)\n",
    "\n",
    "    # Normal Accuracy \n",
    "    train_accuracy = accuracy_score(y_train, train_preds)\n",
    "    test_accuracy  = accuracy_score(y_test, test_preds)\n",
    "\n",
    "    # Balanced Accuracy Metrics\n",
    "    balanced_train_accuracy = balanced_accuracy_score(y_train, train_preds)\n",
    "    balanced_test_accuracy = balanced_accuracy_score(y_test, test_preds)\n",
    "\n",
    "    results_df = pd.DataFrame([{\n",
    "        'model': model_name, \n",
    "        'model_params': model.get_params(),\n",
    "        'mean_cv_accuracy': mean_cv_accuracy,\n",
    "        'std_cv_accuracy': std_cv_accuracy,\n",
    "        'train_accuracy': train_accuracy, \n",
    "        'test_accuracy': test_accuracy,\n",
    "        'balanced_train_accuracy' : balanced_train_accuracy,\n",
    "        'balanced_test_accuracy': balanced_test_accuracy,\n",
    "        'run_comment': run_comment\n",
    "    }])\n",
    "    \n",
    "    if concat_results:\n",
    "        try:\n",
    "            combined_results = pd.concat([combined_results, results_df], ignore_index=True)\n",
    "        except NameError:\n",
    "            combined_results = results_df\n",
    "\n",
    "    return (results_df, model) if return_model else results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf83a89a",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492d6f86",
   "metadata": {},
   "source": [
    "#### **Prepare Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90132c7c",
   "metadata": {},
   "source": [
    "In this following cell we will do two things: \n",
    "\n",
    "1- Train Test Split the data - which will be stored in variables, X_train, X_test, etc. \n",
    "\n",
    "2- Standardize the data and **then** train test split it: This will be stored in variables X_train_scaled, X_test_scaled, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3028e1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non Standardized Data\n",
    "X_train, X_test, y_train, y_test = train_test_split_data(BDB_All_Plays_Model_Ready, 'Inj_Occured')\n",
    "\n",
    "\n",
    "# Standardized Numeric Data\n",
    "BDB_All_Plays_Standardized = standardize_features(BDB_All_Plays_Model_Ready, target_column='Inj_Occured')\n",
    "X_train_Scaled, X_test_scaled, y_train_scaled, y_test_scaled = train_test_split_data(BDB_All_Plays_Standardized, 'Inj_Occured')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56c5f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting PDA_PCA...\n",
      "Done: 5344 train rows, 1337 test rows\n",
      "Splitting FNF_PCA...\n",
      "Done: 213604 train rows, 53402 test rows\n",
      "Splitting BDB_PCA...\n",
      "Done: 6839 train rows, 1710 test rows\n",
      "\n",
      " All splits complete!\n"
     ]
    }
   ],
   "source": [
    "# put all your feature sets into a dictionary\n",
    "feature_sets = {\n",
    "    \"PDA_PCA\": PDA_PCA_Features,\n",
    "    \"FNF_PCA\": FNF_PCA_Features,\n",
    "    \"BDB_PCA\": BDB_PCA_Features,\n",
    "    \"FNF_back\": FNF_back_Features,\n",
    "    \"PDA_back\": PDA_back_Features,\n",
    "    \"BDB_back\": BDB_back_Features,\n",
    "    \"FNF_forward\": FNF_Forward_Features,\n",
    "    \"PDA_forward\": PDA_Forward_Features,\n",
    "    \"BDB_forward\": BDB_Forward_Features\n",
    "}\n",
    "\n",
    "# dictionary to store results\n",
    "splits = {}\n",
    "\n",
    "# loop through and split each\n",
    "for name, df in feature_sets.items():\n",
    "    print(f\"Splitting {name}...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split_data(df, 'Inj_Occured')\n",
    "    splits[name] = {\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test\n",
    "    }\n",
    "    print(f\"Done: {X_train.shape[0]} train rows, {X_test.shape[0]} test rows\")\n",
    "\n",
    "print(\"\\n All splits complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262f1f23",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e534515",
   "metadata": {},
   "source": [
    "# **Modeling**\n",
    "\n",
    "NOTE: As previously discussed in Semester 2, the primary goal of this analysis and modeling excercise is to classify whether a particular play will result in an injury, and to determine the factors that are most likely to cause this injury. Furthermore, as was *also* previously discussed, this dataset has some extreme imbalance issues (injury occurance < 2%), and as such high performance on these baseline models is NOT expected. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e931c075",
   "metadata": {},
   "source": [
    "## **Baseline Logistic Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c73c90",
   "metadata": {},
   "source": [
    "#to do: **put baseline here**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d63bce5",
   "metadata": {},
   "source": [
    "So, in order to help convergence, we cranked the max_iter, and tol hyperparameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75137b5",
   "metadata": {},
   "source": [
    "Great, no more convergence issues. And now, again from week two, we used the scaled feature set to see if that would yield any improvements in performance below. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eebc537",
   "metadata": {},
   "source": [
    "#### **Effect of Standardizing Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6065d82d",
   "metadata": {},
   "source": [
    "____"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfclean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
